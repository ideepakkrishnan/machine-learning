{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.decomposition\n",
    "from skimage.transform import resize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, col_name):\n",
    "    \"\"\"\n",
    "    Load input data from matlab file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : String\n",
    "        The relative path of matlab file    \n",
    "    col_name : String\n",
    "        Label of the input data within the\n",
    "        file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        Contains the resized image data\n",
    "        where each column represents an\n",
    "        image\n",
    "    \n",
    "    N x num_labels matrix\n",
    "        Contains the one-hot encoded form\n",
    "        of each data point's classification\n",
    "    \"\"\"\n",
    "    resize_width = 17\n",
    "    resize_height = 20\n",
    "    \n",
    "    ip = sio.loadmat(path)\n",
    "\n",
    "    N = ip[col_name].shape[1] * ip[col_name][:, 0][0].shape[2]\n",
    "    num_labels = ip[col_name].shape[1]\n",
    "\n",
    "    size = (resize_height, resize_width)\n",
    "    X = np.zeros((N, resize_height * resize_width))\n",
    "    Y = np.zeros((N, num_labels))\n",
    "\n",
    "    img_index = 0\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        curr_class_data = ip[col_name][:,i][0]\n",
    "        for j in range(curr_class_data.shape[2]):\n",
    "            img_resized = resize(curr_class_data[:,:,j], size, mode='constant')\n",
    "            X[img_index, :] = img_resized.flatten()\n",
    "            Y[img_index, i] = 1\n",
    "            img_index += 1\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def cross_entropy_loss_array(probs_array, y_onehot):\n",
    "    indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "    predicted_probability = probs_array[np.arange(len(probs_array)), indices]\n",
    "    log_preds = np.log(predicted_probability)\n",
    "    loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "    return loss\n",
    "\n",
    "def regularization_L2_softmax_loss(reg_lambda, weight1, weight2):\n",
    "    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n",
    "    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n",
    "    return weight1_loss + weight2_loss\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    accuracy = 100.0 * correct_predictions / predictions.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def softmax(output_array):\n",
    "    logits_exp = np.exp(output_array)\n",
    "    return logits_exp / np.sum(logits_exp, axis = 1, keepdims = True)\n",
    "\n",
    "def tanh_derivative(data):\n",
    "    \"\"\"\n",
    "    Computes the derivative after executing\n",
    "    tanh function on the given data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : N x D matrix\n",
    "        The input data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        The tanh derivative of the data\n",
    "    \"\"\"\n",
    "    th = np.tanh(data)\n",
    "    return 1 - th*th\n",
    "\n",
    "def relu(data):\n",
    "    \"\"\"\n",
    "    Applies the ReLU activation function on\n",
    "    data passed as argument\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : N x D matrix\n",
    "        The input data\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        The ReLU applied data\n",
    "    \"\"\"\n",
    "    return np.maximum(data, 0)\n",
    "\n",
    "def relu_derivative(data):\n",
    "    \"\"\"\n",
    "    Applies the derivative of ReLU activation\n",
    "    function on the data as argument\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : N x D matrix\n",
    "        The input data\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        The ReLU derivative applied data\n",
    "    \"\"\"\n",
    "    data[data <= 0] = 0\n",
    "    data[data > 0] = 1\n",
    "    return data\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid activation function: \n",
    "    1 / (1 + e ^ (-z)) where z represents\n",
    "    each data point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : N x D matrix\n",
    "        The data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        The data with sigmoid activation\n",
    "        function applied on it\n",
    "    \"\"\"    \n",
    "    return 1 / (1 + (np.exp(-Z)))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \"\"\"\n",
    "    Applies the derivative of sigmoid\n",
    "    activation function: 1 / (1 + e ^ (-z))\n",
    "    where z represents each data point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : N x D matrix\n",
    "        The data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        The data with sigmoid activation\n",
    "        function applied on it\n",
    "    \"\"\"\n",
    "    partial = 1 / (1 + (np.exp(-Z)))\n",
    "    return np.multiply(partial, (1 - partial))\n",
    "\n",
    "def compute_mse(actual, reconstructed):\n",
    "    \"\"\"\n",
    "    Computes the mean square error while\n",
    "    reconstructing the input from encoded\n",
    "    data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : N x D matrix        \n",
    "        Contains the actual input\n",
    "    reconstructed : N x D matrix\n",
    "        Constains the reconstructed points\n",
    "        from decode layer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double\n",
    "        The calculated MSE\n",
    "    \"\"\"\n",
    "    diff = actual - reconstructed\n",
    "    return np.sum(np.sum(np.square(diff), axis=1), axis=0) / actual.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(X, d):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis on the\n",
    "    given data to reduce its dimensions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : N x D matrix\n",
    "        where N is the number of data points and\n",
    "        D is the ambient dimension\n",
    "    d : Integer\n",
    "        Dimensionality of the low-dimensional\n",
    "        representation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    D x d matrix\n",
    "        The basis of the low-dimensional subspace\n",
    "    1 x d matrix\n",
    "        The mean of the subspace\n",
    "    d x N matrix\n",
    "        The low dimensional representation of the\n",
    "        input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Do mean normalization\n",
    "    M_X = np.sum(X, axis = 0)\n",
    "    M_X = M_X / X.shape[0]\n",
    "    X = X - M_X\n",
    "\n",
    "    # Find the correlation matrix\n",
    "    C = np.dot(X.T, X) / X.shape[0]\n",
    "\n",
    "    # Do eigenvalue decomposition and get hold of \n",
    "    # the eigenvalues (D) and eigenvectors (V) of \n",
    "    # covariance matrix\n",
    "    D, V = np.linalg.eig(C)\n",
    "\n",
    "    # Extract the top-d eigenvectors\n",
    "    V = V[:, 0:d]\n",
    "    \n",
    "    # Represent data in this basis\n",
    "    Y = np.dot(X, V)\n",
    "    \n",
    "    # Calculate the mean of low-dimensional space\n",
    "    M_Y = np.sum(Y, axis=0) / Y.shape[0]\n",
    "    \n",
    "    return V.T, M_Y, Y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "    \n",
    "    def __init__(self, f_activation, f_derivative, n_input, n_hidden, alpha = 0.1, bias = 1.0, l1_penalty = 0.01, mse_threshold = 3):\n",
    "        \n",
    "        # Initialize learning rate and bias\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.mse_threshold = mse_threshold\n",
    "        \n",
    "        # Store the activation function to be used by each neuron\n",
    "        self.f_activation = f_activation\n",
    "        self.f_derivative = f_derivative\n",
    "        \n",
    "        # Store the number of neurons in each layer\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_input\n",
    "        \n",
    "        # Initialize the weights and biases for hidden and output layers\n",
    "        # Dimension of weight matrix for layer l: S[l+1] x S[l]\n",
    "        self.W1 = np.random.normal(0, 1, [self.n_hidden, self.n_input + 1])\n",
    "        self.W2 = np.random.normal(0, 1, [self.n_output, self.n_hidden + 1])\n",
    "        \n",
    "        # Initialize variables to store the input to neuron in each layer\n",
    "        # Dimension of input vector for layer l: S[l]x1\n",
    "        self.z2 = np.zeros((self.n_hidden, 1), dtype=float)\n",
    "        self.z3 = np.zeros((self.n_output, 1), dtype=float)\n",
    "        \n",
    "        # Initialize variables to store the output for neurons in all layers\n",
    "        # Dimension of output vector for layer l: (S[l]+1)x1\n",
    "        self.a1 = np.zeros((self.n_input + 1, 1), dtype=float)\n",
    "        self.a2 = np.zeros((self.n_hidden + 1, 1), dtype=float)\n",
    "        self.a3 = np.zeros((self.n_output, 1), dtype=float)\n",
    "        \n",
    "        # Initialize variables to store the intermediate deltas\n",
    "        # Dimension of delta vectors for layer l: S[l] x 1\n",
    "        self.d2 = np.zeros((self.n_hidden), dtype=float)\n",
    "        self.d3 = np.zeros((self.n_output), dtype=float)        \n",
    "    \n",
    "    def propogate_forward(self, curr_datapoint):\n",
    "        \"\"\"\n",
    "        Performs forward propogation to calculate\n",
    "        the inputs to neurons in the hidden and\n",
    "        output layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        curr_datapoint: a 1xD row vector which has the\n",
    "            first data point\n",
    "        \"\"\"\n",
    "        \n",
    "        # Output from input layer will be the data passed as input\n",
    "        # a1 -> D x 1 vector\n",
    "        self.a1[:-1, 0] = curr_datapoint.T\n",
    "        self.a1[-1:, 0] = self.bias\n",
    "        \n",
    "        # Hidden layer\n",
    "        # z2 -> n_hidden x 1 vector\n",
    "        # a2 -> (n_hidden + 1) x 1 vector\n",
    "        self.z2 = np.dot(self.W1, self.a1)\n",
    "        self.a2[:-1, :] = self.f_activation(self.z2)\n",
    "        self.a2[-1:, :] = self.bias\n",
    "        \n",
    "        # Output layer\n",
    "        # z3 -> n_output x 1 vector\n",
    "        # a3 -> n_output x 1 vector\n",
    "        self.z3 = np.dot(self.W2, self.a2)\n",
    "        self.a3 = self.f_activation(self.z3)\n",
    "    \n",
    "    def propogate_back(self, y):\n",
    "        \"\"\"\n",
    "        Performs backward propogation to calculate\n",
    "        the partial derivative of cost function\n",
    "        with respect to weights and bias terms \n",
    "        using intermediate values calculated during\n",
    "        the forward propogation step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : a n_output x 1 vector\n",
    "            which stores the actual class of\n",
    "            the input\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate delta from output layer\n",
    "        # d3 -> n_output x 1 vector\n",
    "        self.d3 = self.a3 - np.reshape(y, (self.n_output, 1))\n",
    "        \n",
    "        # Calculate delta from hidden layer\n",
    "        # d2 -> n_hidden x 1 vector\n",
    "        z2_derivative = self.f_derivative(self.z2)\n",
    "        self.d2 = np.multiply(\n",
    "                    np.dot(self.W2[:, :-1].T, self.d3), \n",
    "                    z2_derivative)\n",
    "    \n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        Train the neural network with given data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : a N x D matrix\n",
    "            where D is the number of dimensions &\n",
    "            N is the number of data points.\n",
    "            Contains the input data in the form\n",
    "            of a matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (n_hidden + 1) x 1 vector\n",
    "            Stores the weights for inputs of hidden \n",
    "            layer\n",
    "        (n_output + 1) x 1 vector\n",
    "            Stores the weights for inputs of output\n",
    "            layer\n",
    "        List<Double>\n",
    "            The MSEs calculated for every 50th \n",
    "            iteration\n",
    "        \"\"\"\n",
    "        \n",
    "        N = data.shape[0]  # Each row represents a data point\n",
    "        \n",
    "        count = 0\n",
    "        errors = []\n",
    "        while(True):\n",
    "            # Initialize variables to store the change in weight variables\n",
    "            # Dimension of matrix for layer l: S[l+1] x S[l]\n",
    "            D1 = np.matrix(np.zeros((self.n_hidden, self.n_input + 1), dtype=float))\n",
    "            D2 = np.matrix(np.zeros((self.n_output, self.n_hidden + 1), dtype=float))\n",
    "\n",
    "            for i in range(N):\n",
    "                curr_datapoint = data[i, :]\n",
    "\n",
    "                self.propogate_forward(curr_datapoint)\n",
    "                self.propogate_back(curr_datapoint)\n",
    "                \n",
    "                D1[:, :-1] += np.dot(self.d2, self.a1[:-1, :].T)\n",
    "                D1[:, -1:] += self.d2\n",
    "                D2[:, :-1] += np.dot(self.d3, self.a2[:-1, :].T)\n",
    "                D2[:, -1:] += self.d3\n",
    "        \n",
    "            # Update weights\n",
    "            self.W1 -= self.alpha * ((1/N) * D1 + self.l1_penalty * self.W1)\n",
    "            self.W2 -= self.alpha * ((1/N) * D2 + self.l1_penalty * self.W2)\n",
    "            \n",
    "            # Compute MSE for the generated weights\n",
    "            ip_ones = np.ones((data.shape[0], 1))\n",
    "            input_data = np.hstack((data, ip_ones))            \n",
    "            encode_layer = np.dot(input_data, self.W1.T)\n",
    "            encode_layer = self.f_activation(encode_layer)\n",
    "            \n",
    "            decode_ones = np.ones((encode_layer.shape[0], 1))\n",
    "            decode_layer = np.hstack((encode_layer, decode_ones))\n",
    "            decode_layer = np.dot(decode_layer, self.W2.T)\n",
    "            \n",
    "            reconstructed = self.f_activation(decode_layer)\n",
    "            mse = compute_mse(data, reconstructed)\n",
    "            errors.append(mse)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if (mse <= self.mse_threshold):\n",
    "                print(\"Final MSE from training: \", mse)\n",
    "                break\n",
    "                \n",
    "            if (count % 100 == 0):                \n",
    "                print(count, \": \", mse)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        return self.W1, self.W2, encode_layer, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFN:\n",
    "    \n",
    "    def __init__(self, f_activation, f_derivative, n_input, n_hidden, n_output, alpha=0.1, bias=1.0, l1_penalty=0.01):\n",
    "        \n",
    "        # Initialize learning rate and bias\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "        self.l1_penalty = l1_penalty\n",
    "        \n",
    "        # Store the activation function to be used by each neuron\n",
    "        self.f_activation = f_activation\n",
    "        self.f_derivative = f_derivative\n",
    "        \n",
    "        # Store the number of neurons in each layer\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # Initialize the weights and biases for hidden and output layers\n",
    "        # Dimension of weight matrix for layer l: S[l+1] x S[l]\n",
    "        self.W1 = np.random.normal(0, 1, [self.n_hidden, self.n_input + 1])\n",
    "        self.W2 = np.random.normal(0, 1, [self.n_output, self.n_hidden + 1])\n",
    "        \n",
    "        # Initialize variables to store the input to neuron in each layer\n",
    "        # Dimension of input vector for layer l: S[l]x1\n",
    "        self.z2 = np.zeros((self.n_hidden, 1), dtype=float)\n",
    "        self.z3 = np.zeros((self.n_output, 1), dtype=float)\n",
    "        \n",
    "        # Initialize variables to store the output for neurons in all layers\n",
    "        # Dimension of output vector for layer l: (S[l]+1)x1\n",
    "        self.a1 = np.zeros((self.n_input + 1, 1), dtype=float)\n",
    "        self.a2 = np.zeros((self.n_hidden + 1, 1), dtype=float)\n",
    "        self.a3 = np.zeros((self.n_output, 1), dtype=float)\n",
    "        \n",
    "        # Initialize variables to store the intermediate deltas\n",
    "        # Dimension of delta vectors for layer l: S[l] x 1\n",
    "        self.d2 = np.zeros((self.n_hidden), dtype=float)\n",
    "        self.d3 = np.zeros((self.n_output), dtype=float)        \n",
    "    \n",
    "    def propogate_forward(self, curr_datapoint):\n",
    "        \"\"\"\n",
    "        Performs forward propogation to calculate\n",
    "        the inputs to neurons in the hidden and\n",
    "        output layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        curr_datapoint: a 1xD row vector which has the\n",
    "            first data point\n",
    "        \"\"\"\n",
    "        \n",
    "        # Output from input layer will be the data passed as input\n",
    "        # a1 -> D x 1 vector\n",
    "        self.a1[:-1, 0] = curr_datapoint.T\n",
    "        self.a1[-1:, 0] = self.bias\n",
    "        \n",
    "        # Hidden layer\n",
    "        # z2 -> n_hidden x 1 vector\n",
    "        # a2 -> (n_hidden + 1) x 1 vector\n",
    "        self.z2 = np.dot(self.W1, self.a1)\n",
    "        self.a2[:-1, :] = self.f_activation(self.z2)\n",
    "        self.a2[-1:, :] = self.bias\n",
    "        \n",
    "        # Output layer\n",
    "        # z3 -> n_output x 1 vector\n",
    "        # a3 -> n_output x 1 vector\n",
    "        self.z3 = np.dot(self.W2, self.a2)\n",
    "        self.a3 = self.f_activation(self.z3)\n",
    "    \n",
    "    def propogate_back(self, y):\n",
    "        \"\"\"\n",
    "        Performs backward propogation to calculate\n",
    "        the partial derivative of cost function\n",
    "        with respect to weights and bias terms \n",
    "        using intermediate values calculated during\n",
    "        the forward propogation step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : a n_output x 1 vector\n",
    "            which stores the actual class of\n",
    "            the input\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate delta from output layer\n",
    "        # d3 -> n_output x 1 vector\n",
    "        self.d3 = self.a3 - np.reshape(y, (10, 1))\n",
    "        \n",
    "        # Calculate delta from hidden layer\n",
    "        # d2 -> n_hidden x 1 vector\n",
    "        z2_derivative = self.f_derivative(self.z2)\n",
    "        self.d2 = np.multiply(\n",
    "                    np.dot(self.W2[:, :-1].T, self.d3), \n",
    "                    z2_derivative)\n",
    "    \n",
    "    def train(self, data, labels, iterations=5001):\n",
    "        \"\"\"\n",
    "        Train the neural network with given data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : a N x D matrix\n",
    "            where D is the number of dimensions &\n",
    "            N is the number of data points.\n",
    "            Contains the input data in the form\n",
    "            of a matrix\n",
    "        labels : a N x n_output matrix\n",
    "            Contains the true classes of each data\n",
    "            point\n",
    "        iterations : Integer [Optional field]            \n",
    "            The number of iterations of training\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (n_hidden + 1) x 1 vector\n",
    "            Stores the weights for inputs of hidden \n",
    "            layer\n",
    "        (n_output + 1) x 1 vector\n",
    "            Stores the weights for inputs of output\n",
    "            layer\n",
    "        \"\"\"\n",
    "        \n",
    "        N = data.shape[0]  # Each row represents a data point\n",
    "                \n",
    "        for count in range(iterations):\n",
    "            # Initialize variables to store the change in weight variables\n",
    "            # Dimension of matrix for layer l: S[l+1] x S[l]\n",
    "            D1 = np.matrix(np.zeros((self.n_hidden, self.n_input + 1), dtype=float))\n",
    "            D2 = np.matrix(np.zeros((self.n_output, self.n_hidden + 1), dtype=float))\n",
    "\n",
    "            for i in range(N):\n",
    "                curr_datapoint = data[i, :]\n",
    "                curr_datapoint_label = labels[i, :]\n",
    "\n",
    "                self.propogate_forward(curr_datapoint)\n",
    "                self.propogate_back(curr_datapoint_label)\n",
    "                \n",
    "                D1[:, :-1] += np.dot(self.d2, self.a1[:-1, :].T)\n",
    "                D1[:, -1:] += self.d2\n",
    "                D2[:, :-1] += np.dot(self.d3, self.a2[:-1, :].T)\n",
    "                D2[:, -1:] += self.d3\n",
    "        \n",
    "            # Update weights\n",
    "            self.W1 -= self.alpha * ((1/N) * D1 + self.l1_penalty * self.W1)\n",
    "            self.W2 -= self.alpha * ((1/N) * D2 + self.l1_penalty * self.W2)\n",
    "            \n",
    "            # Calculate prediction error\n",
    "            input_layer = np.dot(data, self.W1[:, :-1].T)\n",
    "            hidden_layer = self.f_activation(input_layer)\n",
    "            scores = np.dot(hidden_layer, self.W2[:, :-1].T)\n",
    "            probs = softmax(self.f_activation(scores))\n",
    "            acc = accuracy(probs, labels)\n",
    "            \n",
    "            if (count % 1000 == 0):\n",
    "                print(\"Iteration \", count, \": \", acc)\n",
    "            \n",
    "            # Check for convergence\n",
    "            #fro_norm = np.linalg.norm(D2, 'fro')\n",
    "            #print(fro_norm)\n",
    "        \n",
    "        return self.W1, self.W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_3a(f_activation, f_derivative, n_hidden, iterations=5001):\n",
    "    X, Y = load_data(\"ExtYaleB10.mat\", 'train')\n",
    "    \n",
    "    print(\"Initializind the Feed Forward Neural Network...\")\n",
    "    ffn = FFN(f_activation, f_derivative, X.shape[1], n_hidden, 10)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    wh, wo = ffn.train(X, Y, iterations)\n",
    "    \n",
    "    print(\"Running the model on test data...\")\n",
    "    test_data, test_labels = load_data(\"ExtYaleB10.mat\", 'test')\n",
    "\n",
    "    ip_ones = np.ones((test_data.shape[0], 1))\n",
    "    test_data = np.hstack((test_data, ip_ones))\n",
    "    input_layer = np.dot(test_data, wh.T)\n",
    "\n",
    "    hidden_layer = f_activation(input_layer)\n",
    "    hidden_ones = np.ones((hidden_layer.shape[0], 1))\n",
    "    hidden_layer = np.hstack((hidden_layer, hidden_ones))\n",
    "\n",
    "    scores = np.dot(hidden_layer, wo.T)\n",
    "    scores = f_activation(scores)\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    print('Test accuracy: {0}%'.format(accuracy(probs, test_labels)))\n",
    "\n",
    "def execute_3d(f_activation, f_derivative, n_hidden, iterations=5001):\n",
    "    X, Y = load_data(\"ExtYaleB10.mat\", 'train')\n",
    "    \n",
    "    print(\"Reducing the dimension to 100 using PCA...\")\n",
    "    V_trn, M_trn, X_trn_low_dim = pca(X, 100)\n",
    "    X_trn_low_dim = X_trn_low_dim.T\n",
    "    \n",
    "    print(\"Initializind the Feed Forward Neural Network...\")\n",
    "    ffn = FFN(f_activation, f_derivative, X_trn_low_dim.shape[1], n_hidden, 10)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    wh, wo = ffn.train(X_trn_low_dim, Y, iterations)\n",
    "    \n",
    "    print(\"Running the model on test data...\")\n",
    "    test_data, test_labels = load_data(\"ExtYaleB10.mat\", 'test')\n",
    "    \n",
    "    print(\"Reducing dimensions of test data using PCA...\")\n",
    "    V_tst, M_tst, X_tst_low_dim = pca(test_data, 100)\n",
    "    X_tst_low_dim = X_tst_low_dim.T\n",
    "\n",
    "    ip_ones = np.ones((X_tst_low_dim.shape[0], 1))\n",
    "    X_tst_low_dim = np.hstack((X_tst_low_dim, ip_ones))\n",
    "    input_layer = np.dot(X_tst_low_dim, wh.T)\n",
    "\n",
    "    hidden_layer = f_activation(input_layer)\n",
    "    hidden_ones = np.ones((hidden_layer.shape[0], 1))\n",
    "    hidden_layer = np.hstack((hidden_layer, hidden_ones))\n",
    "\n",
    "    scores = np.dot(hidden_layer, wo.T)\n",
    "    scores = f_activation(scores)\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    print('Test accuracy: {0}%'.format(accuracy(probs, test_labels)))\n",
    "\n",
    "def execute_3e(f_activation, f_derivative, n_hidden=10, alpha=0.1, bias=1.0, l1_penalty=0.01, mse_threshold=3, iterations=5000):\n",
    "\n",
    "    f_activation=np.tanh\n",
    "    f_derivative=tanh_derivative\n",
    "    n_hidden=10\n",
    "    alpha=0.1\n",
    "    bias=1.0\n",
    "    l1_penalty=0.01\n",
    "    mse_threshold=3\n",
    "    iterations=5000\n",
    "\n",
    "    # Train the model\n",
    "    X_trn, Y_trn = load_data(\"ExtYaleB10.mat\", col_name='train')\n",
    "    X_tst, Y_tst = load_data(\"ExtYaleB10.mat\", col_name='test')\n",
    "\n",
    "    # Generate the input and actual output matrices\n",
    "    X = np.vstack((X_trn, X_tst))\n",
    "\n",
    "    print(\"Initializind the Auto Encoder...\")\n",
    "    an = AutoEncoder(f_activation, f_derivative, X.shape[1], 100, alpha, bias, l1_penalty, mse_threshold)\n",
    "    we, wd, ae, mse = an.train(X)\n",
    "    \n",
    "    # Train the Feed Forward Neural Network\n",
    "    X_trn = ae[:X_trn.shape[0], :]\n",
    "\n",
    "    print(\"Initializind the Feed Forward Neural Network...\")\n",
    "    ffn = FFN(relu, relu_derivative, n_input=X_trn.shape[1], n_hidden=50, n_output=10)\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    wh, wo = ffn.train(X_trn, Y_trn, iterations)\n",
    "\n",
    "    print(\"Running the model on test data...\")\n",
    "    X_tst = ae[X_trn.shape[0]:, :]\n",
    "\n",
    "    ip_ones = np.ones((X_tst.shape[0], 1))\n",
    "    X_tst = np.hstack((X_tst, ip_ones))\n",
    "    input_layer = np.dot(X_tst, wh.T)\n",
    "\n",
    "    hidden_layer = f_activation(input_layer)\n",
    "    hidden_ones = np.ones((hidden_layer.shape[0], 1))\n",
    "    hidden_layer = np.hstack((hidden_layer, hidden_ones))\n",
    "\n",
    "    scores = np.dot(hidden_layer, wo.T)\n",
    "    scores = f_activation(scores)\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    print('Test accuracy: {0}%'.format(accuracy(probs, Y_tst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializind the Feed Forward Neural Network...\n",
      "Training the model...\n",
      "Iteration  0 :  9.0\n",
      "Iteration  1000 :  33.2\n",
      "Iteration  2000 :  96.0\n",
      "Iteration  3000 :  99.6\n",
      "Iteration  4000 :  98.0\n",
      "Iteration  5000 :  99.2\n",
      "Running the model on test data...\n",
      "Test accuracy: 94.28571428571429%\n"
     ]
    }
   ],
   "source": [
    "execute_3a(f_activation=np.tanh, f_derivative=tanh_derivative, n_hidden=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "execute_3d(f_activation=np.tanh, f_derivative=tanh_derivative, n_hidden=50, iterations=2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
